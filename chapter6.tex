\chapter{Experimental Results}

In this chapter, we present the experimental results of our research. As no previous work has attempted to develop models for automatic pain assessment in fetuses, we have no baseline to compare to. Thus, we discuss the decisions we have made along the way, which led to our best results. In particular, our experiments aim to answer the following research questions:

\begin{itemize}
    \item \textbf{RQ1:} Is it possible to construct a learning model capable of identifying the presence of pain in images of fetuses?
    
    \item \textbf{RQ2:} Is this model capable of discriminating images of fetuses while in acute pain exposure from those in a control group while in rest our in a vibroacoustic sound stimulus?
    
    \item \textbf{RQ3:} Does transfer learning transfer well from face recognition tasks in adults to our domain?
\end{itemize}

\section{Setup}

Given we had relatively few data available, we chose a validation strategy that works best in this scenario. Like \cite{CelonaM17}, we used the leave-one-out method for cross-validation, but instead of leaving one image out, we leave one subject. Hence, we produce 15 different combinations containing training and test subsets. On each of these combinations, we train our networks in the training subset and evaluate on the test subset. All the evaluations are then averaged to assess the overall performance of the models.

The metric we used to evaluate our model in each validation set was accuracy. To calculate it for a given test set, we divide the number of images we have predicted the correct class by the total number of images available in that set.

Besides that, we have also calculated another metric for the videos of acute pain. As we have 45 seconds of video before the acute pain stimulus, and 45 seconds after it, we have images from both classes in these videos: Pain and Non-Pain.  This division allows the use of a metric that considers not only the cases we are making the correct prediction but also how much of each class we are making wrong predictions. Thus, like \cite{abs-1807-01631}, we have used the Area Under the ROC Curve (AUC) to evaluate the effectiveness of our models.

% \subsection{Tooling}
% \subsection{Metric}
% \subsection{Validation}
% \subsection{Error Analysis}

\section{Results}


